{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Template\n",
    "\n",
    "This is a template homework for CS498 Deep Learning for Healthcare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# set seed\n",
    "seed = 7\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 ECG Data Classification [? points]\n",
    "\n",
    "In this section, you will implement an advanced CNN+RNN model with attention mechanism to classify ECG recordings. Specifically, we face a binary classification problem, and the goal is to distinguish atrial fibrillation(AF), an alternative rhythm, from the normal sinus rhythm. We will be using a fraction of the data in the public [Physionet 2017 Challenge](https://physionet.org/content/challenge-2017/1.0.0/). More details can be found in the link.\n",
    "\n",
    "ECG recordings were sampled at 300Hz, and for the purpose of this task, the data we use is separated into 10-second-segments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load the Data\n",
    "\n",
    "Because the preprocessing of the data requires a tremendous amount of memory and time, for the sake of this homework, the data has already been preprocessed. \n",
    "\n",
    "Specifically, for each raw data (an ECG recording sampled at 300Hz), we did the following:\n",
    "1. split the dataset into training/validation/test sets with a ratio of [placeholder]\n",
    "2. for each recording, we normalize the data to have a mean of 0 and a standard deviation of 1\n",
    "3. slide and cut the recording into overlapping 10-second-segments (stride =500 for class 0, 50 for class 1 to oversample).\n",
    "4. use FIR bandpass filter to transform the data from 1 channel to 4 channels.\n",
    "\n",
    "Due to the resource constraints, the data and knowledge features have already been computed. You will need to write a dataloader for the training and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1696 training data, 425 test data\n",
      "Shape of X: (4, 3000) = (#channels, n)\n",
      "Shape of beat feature: (4, 3000) = (#channels, n)\n",
      "Shape of rhythm feature: (4, 60) = (#channels, M)\n",
      "Shape of frequency feature: (4, 1) = (#channels, 1)\n"
     ]
    }
   ],
   "source": [
    "data_path = r'G:\\MINA\\data\\challenge2017\\100_cached_data_permuted7'\n",
    "train_dict = pd.read_pickle(os.path.join(data_path, 'train.pkl'))\n",
    "test_dict = pd.read_pickle(os.path.join(data_path, 'test.pkl'))\n",
    "\n",
    "print(f\"There are {len(train_dict['Y'])} training data, {len(test_dict['Y'])} test data\")\n",
    "print(f\"Shape of X: {train_dict['X'][:, 0,:].shape} = (#channels, n)\")\n",
    "print(f\"Shape of beat feature: {train_dict['K_beat'][:, 0, :].shape} = (#channels, n)\")\n",
    "print(f\"Shape of rhythm feature: {train_dict['K_rhythm'][:, 0, :].shape} = (#channels, M)\")\n",
    "print(f\"Shape of frequency feature: {train_dict['K_freq'][:, 0, :].shape} = (#channels, 1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ECGDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_dict):\n",
    "        \"\"\"\n",
    "        TODO: init the Dataset instance.\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        self.X, self.Y, self.K_beat, self.K_thythm, self.K_freq = data_dict['X'], data_dict['Y'], data_dict['K_beat'], data_dict['K_rhythm'], data_dict['K_freq']\n",
    "        ### END SOLUTION\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        TODO: Denotes the total number of samples\n",
    "        \"\"\"\n",
    "\n",
    "        ### BEGIN SOLUTION\n",
    "        return len(self.Y)\n",
    "        ### END SOLUTION\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"\n",
    "        TODO: Generates one sample of data\n",
    "            return the ((X, K_beat, K_rhythm, K_freq), Y) for the i-th data.\n",
    "            Be careful about which dimension you are indexing.\n",
    "        \"\"\"\n",
    "\n",
    "        ### BEGIN SOLUTION\n",
    "        return (self.X[:, i, :], self.K_beat[:, i, :], self.K_thythm[:, i, :], self.K_freq[:, i, :]), self.Y[i]\n",
    "        ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "def load_data(dataset, batch_size=128):\n",
    "    \"\"\"\n",
    "    Return a DataLoader instance basing on a Dataset instance, with batch_size specified.\n",
    "    Note that since the data has already been shuffled, we set shuffle=False\n",
    "    \"\"\"\n",
    "    def my_collate(batch):\n",
    "        \"\"\"\n",
    "        :param batch: this is essentially [dataset[i] for i in [...]]\n",
    "        batch[i] should be ((Xi, Ki_beat, Ki_rhythm, Ki_freq), Yi)\n",
    "        TODO: write a collate function such that it outputs ((X, K_beat, K_rhythm, K_freq), Y)\n",
    "            each output variable is a batched version of what's in the input *batch*\n",
    "            For each output variable - it should be either float tensor or long tensor (for Y). If applicable, channel dim precedes batch dim\n",
    "            e.g. the shape of each Xi is (# channels, n). In the output, X should be of shape (batch_size, # channels, n)\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        X = torch.tensor([[_x[0][0][c] for _x in batch] for c in range(4)], dtype=torch.float)\n",
    "        K_beat = torch.tensor([[_x[0][1][c] for _x in batch] for c in range(4)], dtype=torch.float)\n",
    "        K_rhythm = torch.tensor([[_x[0][2][c] for _x in batch] for c in range(4)], dtype=torch.float)\n",
    "        K_freq = torch.tensor([[_x[0][3][c] for _x in batch] for c in range(4)], dtype=torch.float)\n",
    "        Y = torch.tensor([_x[1] for _x in batch], dtype=torch.long)\n",
    "        ### END SOLUTION\n",
    "        return (X, K_beat, K_rhythm, K_freq), Y\n",
    "\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=my_collate)\n",
    "\n",
    "\n",
    "train_loader = load_data(ECGDataset(train_dict))\n",
    "test_loader = load_data(ECGDataset(test_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Model Defintions [? points]\n",
    "\n",
    "Now, let us implement a model that involves RNN, CNN and attention mechanism. More specifically, we will implement [MINA: Multilevel Knowledge-Guided Attention for Modeling Electrocardiography Signals](https://www.ijcai.org/Proceedings/2019/0816.pdf).\n",
    "\n",
    "### 3.1 Knowledge-guided attention \n",
    "[Placeholder for explanation] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T13:43:24.044942Z",
     "start_time": "2020-11-24T13:43:24.042214Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class KnowledgeAttn(nn.Module):\n",
    "    def __init__(self, input_features, attn_dim):\n",
    "        \"\"\"\n",
    "        This is the general knowledge-guided attention module.\n",
    "        It will transform the input and knowledge with 2 linear layers, computes attention, and then aggregate.\n",
    "        :param input_features: the number of features for each\n",
    "        :param attn_dim: the number of hidden nodes in the attention mechanism\n",
    "        TODO:\n",
    "            define the following 2 linear layers WITHOUT bias (with the names provided)\n",
    "                att_W: a Linear layer of shape (input_features + n_knowledge, attn_dim)\n",
    "                att_v: a Linear layer of shape (attn_dim, 1)\n",
    "            init the weights using self.init() (already given)\n",
    "        \"\"\"\n",
    "        super(KnowledgeAttn, self).__init__()\n",
    "        self.input_features = input_features\n",
    "        self.attn_dim = attn_dim\n",
    "        self.n_knowledge = 1\n",
    "\n",
    "        ### BEGIN SOLUTION\n",
    "        self.att_W = nn.Linear(self.input_features + self.n_knowledge, self.attn_dim, bias=False)\n",
    "        self.att_v = nn.Linear(self.attn_dim, 1, bias=False)\n",
    "        ### END SOLUTION\n",
    "\n",
    "        self.init()\n",
    "\n",
    "    def init(self):\n",
    "        nn.init.normal_(self.att_W.weight)\n",
    "        nn.init.normal_(self.att_v.weight)\n",
    "\n",
    "    @classmethod\n",
    "    def attention_sum(cls, x, attn):\n",
    "        \"\"\"\n",
    "\n",
    "        :param x: of shape (-1, D, nfeatures)\n",
    "        :param attn: of shape (-1, D, 1)\n",
    "        TODO: return the weighted sum of x along the middle axis with weights even in attn. output shoule be (-1, nfeatures)\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        return torch.sum(torch.mul(attn, x), 1)\n",
    "        ### END SOLUTION\n",
    "\n",
    "\n",
    "    def forward(self, x, k):\n",
    "        \"\"\"\n",
    "        :param x: shape of (-1, D, input_features)\n",
    "        :param k: shape of (-1, D, 1)\n",
    "        :return:\n",
    "            out: shape of (-1, input_features), the aggregated x\n",
    "            attn: shape of (-1, D, 1)\n",
    "        TODO:\n",
    "            concatenate the input x and knowledge k together (on the last dimension)\n",
    "            pass the concatenated output through the learnable Linear transforms\n",
    "                first att_W, then tanh, then att_v\n",
    "                the output shape should be (-1, D, 1)\n",
    "            to get attention values, apply softmax on the output of linear layer\n",
    "                You could use F.softmax(). Be careful which dimension you apply softmax over\n",
    "            aggregate x using the attention values via self.attention_sum, and return\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        tmp = torch.cat([x, k], dim=-1)\n",
    "        e = self.att_v(torch.tanh(self.att_W(tmp)))\n",
    "        attn = F.softmax(e, 1)\n",
    "        out = self.attention_sum(x, attn)\n",
    "        ### END SOLUTION\n",
    "        return out, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n",
    "def float_tensor_equal(a, b, eps=1e-3):\n",
    "    return torch.norm(a-b).abs().max().tolist() < eps\n",
    "\n",
    "def testKnowledgeAttn():\n",
    "    m = KnowledgeAttn(2, 2)\n",
    "    m.att_W.weight.data = torch.tensor([[0.3298,  0.7045, -0.1067],\n",
    "                                        [0.9656,  0.3090,  1.2627]], requires_grad=True)\n",
    "    m.att_v.weight.data = torch.tensor([[-0.2368,  0.5824]], requires_grad=True)\n",
    "\n",
    "    x = torch.tensor([[[-0.6898, -0.9098], [0.0230,  0.2879], [-0.2534, -0.3190]],\n",
    "                      [[ 0.5412, -0.3434], [0.0289, -0.2837], [-0.4120, -0.7858]]])\n",
    "    k = torch.tensor([[ 0.5469,  0.3948, -1.1430], [0.7815, -1.4787, -0.2929]]).unsqueeze(2)\n",
    "    out, attn = m(x, k)\n",
    "\n",
    "    tout = torch.tensor([[-0.2817, -0.2531], [0.2144, -0.4387]])\n",
    "    tattn = torch.tensor([[[0.3482], [0.4475], [0.2043]],\n",
    "                          [[0.5696], [0.1894], [0.2410]]])\n",
    "    assert float_tensor_equal(attn, tattn), \"The attention values are wrong\"\n",
    "    assert float_tensor_equal(out, tout), \"output of the attention module is wrong\"\n",
    "testKnowledgeAttn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "MINA has three different knowledge guided attention mechanisms:\n",
    " - Beat Level $K_\\alpha$: extract beat knowledge which is represented by the first-order difference and a convolutional operation $Conv_\\alpha$ for each segment\n",
    " - Rhythm Level $K_\\beta$: extract rhythm features represented by the standard deviation on each segment\n",
    " - Frequency Level $K_\\gamma$: frequency features are represented by the power spectral density (PSD), which is a popular measure of energy in signal processing.\n",
    " \n",
    "As a result, let's define three Modules that handle Beat/Rhythm/Frequency level information separately. Note that although the input has 4 channels, we actually need to handle each channel separately because they have different meanings after we did the FIR. Thus, we will need 4 BeatNet, 4 RhythmNet, and 1 FreqNet. The pipeline is the following:\n",
    "    BeatNet_i -> RhythmNet_i -> FreqNet\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeatNet(nn.Module):\n",
    "    #Attention for the CNN step/ beat level/local information\n",
    "    def __init__(self, n=3000, T=50,\n",
    "                 conv_out_channels=64):\n",
    "        \"\"\"\n",
    "        :param n: size of each 10-second-data\n",
    "        :param T: size of each smaller segment used to capture local information in the CNN stage\n",
    "        :param conv_out_channels: also called number of filters/kernels\n",
    "        TODO: We will define a network that does two things. Specifically:\n",
    "            1. use one 1-D convolutional layer to capture local informatoin, on x and k_beat (see forward())\n",
    "                conv: The kernel size should be set to 32, and the number of filters should be set to *conv_out_channels*. Stride should be *conv_stride*\n",
    "                conv_k: same as conv, except that it has only 1 filter instead of *conv_out_channels*\n",
    "            2. an attention mechanism to aggregate the convolution outputs. Specifically:\n",
    "                attn: KnowledgeAttn with input_features equaling conv_out_channels, and attn_dim=att_cnn_dim\n",
    "        \"\"\"\n",
    "        super(BeatNet, self).__init__()\n",
    "        self.n, self.M, self.T = n, int(n/T), T\n",
    "        self.conv_out_channels = conv_out_channels\n",
    "        self.conv_kernel_size = 32\n",
    "        self.conv_stride = 2\n",
    "        ### BEGIN SOLUTION\n",
    "        self.conv = nn.Conv1d(in_channels=1,\n",
    "                              out_channels=self.conv_out_channels,\n",
    "                              kernel_size=self.conv_kernel_size,\n",
    "                              stride=self.conv_stride)\n",
    "\n",
    "        self.conv_k = nn.Conv1d(in_channels=1,\n",
    "                                out_channels=1,\n",
    "                                kernel_size=self.conv_kernel_size,\n",
    "                                stride=self.conv_stride)\n",
    "        ### END SOLUTION\n",
    "\n",
    "        self.att_cnn_dim = 8\n",
    "        ### BEGIN SOLUTION\n",
    "        self.attn = KnowledgeAttn(self.conv_out_channels, self.att_cnn_dim)\n",
    "        ### END SOLUTION\n",
    "\n",
    "    def forward(self, x, k_beat):\n",
    "        \"\"\"\n",
    "        :param x: shape (batch, n)\n",
    "        :param k_beat: shape (batch, n)\n",
    "        :return:\n",
    "            out: shape (batch * M, T)\n",
    "            alpha: shape (batch * M, N, 1) where N is a result of convolution\n",
    "        TODO:\n",
    "            reshape the data - convert x/k_beat of shape (batch, n) to (batch * M, 1, T), where n = MT\n",
    "            apply convolution on x and k_beat\n",
    "                pass the reshaped x through self.conv, and then ReLU\n",
    "                pass the reshaped k_beat through self.conv_k, and then ReLU\n",
    "            (at this step, you might need to swap axes to align the dimensions depending on how you defined the layers)\n",
    "            pass the conv'd x and conv'd knowledge through attn to get the output (*out*) and alpha\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        x = x.view(-1, self.T).unsqueeze(1)\n",
    "        k_beat = k_beat.view(-1, self.T).unsqueeze(1)\n",
    "\n",
    "        x = F.relu(self.conv(x))  # Here number of filters K=64\n",
    "        k_beat = F.relu(self.conv_k(k_beat))  # Conv1d(1, 1, kernel_size=(32,), stride=(2,)) => k_beat:[128*60,1,10].\n",
    "\n",
    "        x = x.permute(0, 2, 1)  # x:[128*60,10,64]\n",
    "        k_beat = k_beat.permute(0, 2, 1)\n",
    "        out, alpha = self.attn(x, k_beat)\n",
    "        ### END SOLUTION\n",
    "        return out, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RhythmNet(nn.Module):\n",
    "    def __init__(self, n=3000, T=50, input_size=64, rhythm_out_size=8):\n",
    "        \"\"\"\n",
    "        :param n: size of each 10-second-data\n",
    "        :param T: size of each smaller segment used to capture local information in the CNN stage\n",
    "        :param input_size: This is the same as the # of filters/kernels in the CNN part.\n",
    "        :param rhythm_out_size: output size of this netowrk\n",
    "        TODO: We will define a network that does two things to handle rhythms. Specifically:\n",
    "            1. use a bi-directional LSTM to process the learned local representations from the CNN part\n",
    "                lstm: bidirectional, 1 layer, batch_first, and hidden_size should be set to *rnn_hidden_size*\n",
    "            2. an attention mechanism to aggregate the convolution outputs. Specifically:\n",
    "                attn: KnowledgeAttn with input_features equaling lstm output, and attn_dim=att_rnn_dim\n",
    "            3. output layers\n",
    "                fc: a Linear layer making the output of shape (..., self.out_size)\n",
    "                do: a Dropout layer with p=0.5\n",
    "        \"\"\"\n",
    "        #input_size is the cnn_out_channels\n",
    "        super(RhythmNet, self).__init__()\n",
    "        self.n, self.M, self.T = n, int(n/T), T\n",
    "        self.input_size = input_size\n",
    "\n",
    "        ### LSTM Input: (batch size, M, input_size)\n",
    "        self.rnn_hidden_size = 32\n",
    "        ### BEGIN SOLUTION\n",
    "        self.lstm = nn.LSTM(input_size=self.input_size, #self.conv_out_channels,\n",
    "                            hidden_size=self.rnn_hidden_size,\n",
    "                            num_layers=1, batch_first=True, bidirectional=True)\n",
    "        ### END SOLUTION\n",
    "\n",
    "        ### Attention mechanism\n",
    "        self.att_rnn_dim = 8\n",
    "        ### BEGIN SOLUTION\n",
    "        self.attn = KnowledgeAttn(2 * self.rnn_hidden_size, self.att_rnn_dim)\n",
    "        ### END SOLUTION\n",
    "\n",
    "        ### Dropout and fully connecte layers\n",
    "        self.out_size = rhythm_out_size\n",
    "        ### BEGIN SOLUTION\n",
    "        self.fc = nn.Linear(2 * self.rnn_hidden_size, self.out_size)\n",
    "        self.do = nn.Dropout(p=0.5)\n",
    "        ### END SOLUTION\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, k_rhythm):\n",
    "        \"\"\"\n",
    "        :param x: shape (batch * M, self.input_size=T)\n",
    "        :param k_rhythm: shape (batch, M)\n",
    "        :return:\n",
    "            out: shape (batch, self.out_size)\n",
    "            beta: shape (batch, M, 1)\n",
    "        TODO:\n",
    "            reshape the data - convert x to of shape (batch, M, self.input_size), k_rhythm->(batch, M, 1)\n",
    "            pass the reshaped x through lstm\n",
    "            pass the lstm output and knowledge through attn\n",
    "            pass the result through fully connected layer - ReLU - Dropout\n",
    "            denote the final output as *out*\n",
    "        \"\"\"\n",
    "\n",
    "        ### BEGIN SOLUTION\n",
    "        ### reshape for rnn\n",
    "        self.batch_size = int(x.size()[0] / self.M)\n",
    "        x = x.view(self.batch_size, self.M, -1)\n",
    "        ### rnn\n",
    "        k_rhythm = k_rhythm.unsqueeze(-1)  # [128, 60, 1]\n",
    "        o, (ht, ct) = self.lstm(x)  # o:[batch,60,64] (in the paper this is called h\n",
    "\n",
    "        x, beta = self.attn(o, k_rhythm)\n",
    "        ### fc and Dropout\n",
    "        x = F.relu(self.fc(x))  # [128, 64->8]\n",
    "        out = self.do(x)\n",
    "        ### END SOLUTION\n",
    "        return out, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FreqNet(nn.Module):\n",
    "    def __init__(self, n_channels=4, n=3000, T=50):\n",
    "        \"\"\"\n",
    "        :param n_channels: number of channels (F in the paper). We will need to define this many BeatNet & RhythmNet nets.\n",
    "        :param n: size of each 10-second-data\n",
    "        :param T: size of each smaller segment used to capture local information in the CNN stage\n",
    "        TODO: This is the main network that orchestrates the previously defined attention modules:\n",
    "            1. define n_channels many BeatNet and RhythmNet modules. (Hint: use nn.ModuleList)\n",
    "                beat_nets: for each beat_net, pass parameter conv_out_channel into the init()\n",
    "                rhythm_nets: for each rhythm_net, pass conv_out_channel as input_size, and self.rhythm_out_size as the output size\n",
    "            2. define frequency (channel) level knowledge-guided attention module\n",
    "                attn: KnowledgeAttn with input_features equaling rhythm_out_size, and attn_dim=att_channel_dim\n",
    "            3. output layer: a Linear layer for 2 classes output\n",
    "        \"\"\"\n",
    "        super(FreqNet, self).__init__()\n",
    "        self.n, self.M, self.T = n, int(n / T), T\n",
    "        self.n_class = 2\n",
    "        self.n_channels = n_channels\n",
    "        self.conv_out_channels=64\n",
    "        self.rhythm_out_size=8\n",
    "\n",
    "        ### BEGIN SOLUTION\n",
    "        self.beat_nets = nn.ModuleList()\n",
    "        self.rhythm_nets = nn.ModuleList()\n",
    "        for channel_i in range(self.n_channels):\n",
    "            self.beat_nets.append(BeatNet(self.n, self.T, self.conv_out_channels))\n",
    "            self.rhythm_nets.append(RhythmNet(self.n, self.T, self.conv_out_channels, self.rhythm_out_size))\n",
    "        ### END SOLUTION\n",
    "\n",
    "        ### frequency attention\n",
    "        self.att_channel_dim = 2\n",
    "        ### BEGIN SOLUTION\n",
    "        self.attn = KnowledgeAttn(self.rhythm_out_size, self.att_channel_dim)\n",
    "        ### END SOLUTION\n",
    "\n",
    "        ### fully-connected output layer\n",
    "        ### BEGIN SOLUTION\n",
    "        self.fc = nn.Linear(self.rhythm_out_size, self.n_class)\n",
    "        ### END SOLUTION\n",
    "\n",
    "\n",
    "    def forward(self, x, k_beats, k_rhythms, k_freq):\n",
    "        \"\"\"\n",
    "        We need to use the attention submodules to process data from each channel separately, and then pass the\n",
    "            output through an attention on frequency for the final output\n",
    "\n",
    "        :param x: shape (n_channels, batch, n)\n",
    "        :param k_beats: (n_channels, batch, n)\n",
    "        :param k_rhythms: (n_channels, batch, M)\n",
    "        :param k_freq: (n_channels, batch, 1)\n",
    "        :return:\n",
    "            out: softmax output for each data point, shpae (batch, n_class)\n",
    "        TODO:\n",
    "            1. pass each channel of x through the corresponding beat_net, then rhythm_net.\n",
    "                We will discard the attention (alpha and beta) outputs for now\n",
    "            2. stack the output from 1 together into a tensor of shape (batch, n_channels, rhythm_out_size)\n",
    "            3. pass result from 2 and k_freq through attention module, to get the aggregated result and gama\n",
    "            4. pass aggregated result from 3 through the final fully connected layer.\n",
    "            5. Apply Softmax to normalize output to a probability distribution (over 2 classes)\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        new_x = [None for _ in range(self.n_channels)]\n",
    "        att_dic = {}\n",
    "        for i in range(self.n_channels):\n",
    "            tx, att_dic['alpha_%d'%i] = self.beat_nets[i](x[i], k_beats[i])\n",
    "            new_x[i], att_dic['beta_%d'%i] = self.rhythm_nets[i](tx, k_rhythms[i])\n",
    "        x = torch.stack(new_x, 1)  # [128,8] -> [128,4,8]\n",
    "\n",
    "        # ### attention on channel\n",
    "        k_freq = k_freq.permute(1, 0, 2) #[4,128,1] -> [128,4,1]\n",
    "        x, gama = self.attn(x, k_freq)\n",
    "\n",
    "        ### fc\n",
    "        out = F.softmax(self.fc(x), 1)\n",
    "\n",
    "        ### return\n",
    "        att_dic['gama'] = gama\n",
    "        ### END SOLUTION\n",
    "        return out, gama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation\n",
    "In this part we will define the training procedures, train the model, and evaluate the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, n_epoch=5, lr=0.003, device=None):\n",
    "    import torch.optim as optim\n",
    "    \"\"\"\n",
    "    :param model: The instance of FreqNet that we are training\n",
    "    :param train_dataloader: the DataLoader of the training data\n",
    "    :param n_epoch: number of epochs to train\n",
    "    :return:\n",
    "        model: trained model\n",
    "        loss_history: recorded training loss history - should be just a list of float\n",
    "    TODO:\n",
    "        Specify the optimizer to be optim.Adam\n",
    "        Specify the loss function to be CrossEntropyLoss\n",
    "        Hint: to use dataloader, you can do:\n",
    "            for (X, K_beat, K_rhythm, K_freq), Y in train_dataloader:\n",
    "                ....\n",
    "\n",
    "    \"\"\"\n",
    "    device = device or torch.device('cpu')\n",
    "    model.train()\n",
    "\n",
    "    loss_history = []\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    from tqdm import tqdm\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "    for epoch in range(n_epoch):\n",
    "        curr_epoch_loss = []\n",
    "        for (X, K_beat, K_rhythm, K_freq), Y in tqdm(train_dataloader, desc='train', ncols=80):\n",
    "            X, K_beat, K_rhythm, K_freq, Y = X.to(device), K_beat.to(device), K_rhythm.to(device), K_freq.to(device), Y.to(device)\n",
    "            pred, _ = model(X, K_beat, K_rhythm, K_freq)\n",
    "            loss = loss_func(pred, Y)\n",
    "            curr_epoch_loss.append(loss.cpu().data.numpy())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"epoch{epoch}: curr_epoch_loss={np.mean(curr_epoch_loss)}\")\n",
    "        loss_history += curr_epoch_loss\n",
    "    ### END SOLUTION\n",
    "    return model, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, dataloader, device=None):\n",
    "    \"\"\"\n",
    "    :return:\n",
    "        pred_all: prediction of model on the dataloder.\n",
    "            Should be an 2D numpy float array where the second dimension has length 2.\n",
    "        Y_test: truth labels. Should be an numpy array of ints\n",
    "    TODO:\n",
    "        evaluate the model using on the data in the dataloder.\n",
    "        Add all the prediction and truth to the corresponding list\n",
    "        Convert pred_all and Y_test to numpy arrays.\n",
    "    \"\"\"\n",
    "    device = device or torch.device('cpu')\n",
    "    model.eval()\n",
    "    pred_all = []\n",
    "    Y_test = []\n",
    "    ### BEGIN SOLUTION\n",
    "    from tqdm import tqdm\n",
    "    for (X, K_beat, K_rhythm, K_freq), Y in tqdm(dataloader, desc='test', ncols=80):\n",
    "        X, K_beat, K_rhythm, K_freq, Y = X.to(device), K_beat.to(device), K_rhythm.to(device), K_freq.to(device), Y.to(device)\n",
    "\n",
    "        pred, _ = model.forward(X, K_beat, K_rhythm, K_freq)\n",
    "\n",
    "        pred_all.append(pred.cpu().data.numpy())\n",
    "        Y_test.append(Y.cpu().data.numpy())\n",
    "    pred_all = np.concatenate(pred_all, axis=0)\n",
    "    Y_test = np.concatenate(Y_test, axis=0)\n",
    "    ### END SOLUTION\n",
    "\n",
    "    return pred_all, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|████████████████████████████████████| 14/14 [00:21<00:00,  1.55s/it]\n",
      "train:   0%|                                             | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0: curr_epoch_loss=0.6898131370544434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|████████████████████████████████████| 14/14 [00:20<00:00,  1.49s/it]\n",
      "train:   0%|                                             | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1: curr_epoch_loss=0.6576957106590271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|████████████████████████████████████| 14/14 [00:20<00:00,  1.45s/it]\n",
      "train:   0%|                                             | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch2: curr_epoch_loss=0.5675951242446899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|████████████████████████████████████| 14/14 [00:21<00:00,  1.56s/it]\n",
      "train:   0%|                                             | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch3: curr_epoch_loss=0.503008246421814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|████████████████████████████████████| 14/14 [00:19<00:00,  1.42s/it]\n",
      "test:   0%|                                               | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch4: curr_epoch_loss=0.47429385781288147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|███████████████████████████████████████| 4/4 [00:03<00:00,  1.33it/s]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "n_epoch = 5\n",
    "lr = 0.003\n",
    "n_channel = 4\n",
    "n_dim=3000\n",
    "T=50\n",
    "\n",
    "model = FreqNet(n_channel, n_dim, T)\n",
    "model = model.to(device)\n",
    "\n",
    "model, loss_history = train_model(model, train_loader, n_epoch=n_epoch, lr=lr, device=device)\n",
    "pred, truth = eval_model(model, test_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_predictions(truth, pred):\n",
    "    \"\"\"\n",
    "    TODO: Evaluate the performance of the predictoin via AUROC, AUPRC, and F1 score\n",
    "\n",
    "    each prediction in pred is a vector representing [p_0, p_1].\n",
    "    When defining the scores we are interesed in detecting class 1 only\n",
    "    (Hint: use roc_auc_score, average_precision_score, f1_score from sklearn.metrics)\n",
    "    return: auroc, auprc, f1\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import roc_auc_score, average_precision_score, f1_score\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    pred_label = []\n",
    "    for i in pred:\n",
    "        pred_label.append(np.argmax(i))\n",
    "    pred_label = np.array(pred_label)\n",
    "    auroc = roc_auc_score(truth, pred[:, 1])\n",
    "    auprc = average_precision_score(truth, pred[:, 1])\n",
    "    f1 = f1_score(truth, pred_label)\n",
    "    ### END SOLUTION\n",
    "\n",
    "    return auroc, auprc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC=0.9107982261640798, AUPRC=0.8887289211695684, F1=0.8693586698337292\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "auroc, auprc, f1 = evaluate_predictions(truth, pred)\n",
    "print(f\"AUROC={auroc}, AUPRC={auprc}, F1={f1}\")\n",
    "\n",
    "assert auroc > 0.85 and f1 > 0.8, \"Performance is too low. Something's probably off.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
